{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST training using CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heayounchoi/anaconda3/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost 0.250495911\n",
      "Epoch: 0002 cost 0.068430156\n",
      "Epoch: 0003 cost 0.048835058\n",
      "Epoch: 0004 cost 0.039090190\n",
      "Epoch: 0005 cost 0.032207254\n",
      "Epoch: 0006 cost 0.026936965\n",
      "Epoch: 0007 cost 0.022801897\n",
      "Epoch: 0008 cost 0.018768203\n",
      "Epoch: 0009 cost 0.015232667\n",
      "Epoch: 0010 cost 0.012859721\n",
      "Epoch: 0011 cost 0.010880950\n",
      "Epoch: 0012 cost 0.010811281\n",
      "Epoch: 0013 cost 0.008747036\n",
      "Epoch: 0014 cost 0.006726051\n",
      "Epoch: 0015 cost 0.006639170\n",
      "Accuracy:  tf.Tensor(0.9875, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "x_train = tf.reshape(x_train, [-1, 28, 28, 1])\n",
    "x_test = tf.reshape(x_test, [-1, 28, 28, 1])\n",
    "y_train = tf.one_hot(y_train, depth=nb_classes)\n",
    "y_test = tf.one_hot(y_test, depth=nb_classes)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "dataset = train_data.batch(batch_size)\n",
    "\n",
    "# for features, label in dataset.take(1):\n",
    "    # print(features, label)\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "W1 = tf.Variable(initializer([3, 3, 1, 32]))\n",
    "\n",
    "W2 = tf.Variable(initializer([3, 3, 32, 64]))\n",
    "\n",
    "W3 = tf.Variable(initializer([7*7*64, nb_classes]))\n",
    "b = tf.Variable(tf.random.normal([nb_classes]))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def compute_cost(X, Y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    L1 = tf.nn.relu(L1)\n",
    "    L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    \n",
    "    L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    L2 = tf.nn.relu(L2)\n",
    "    L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    L2 = tf.reshape(L2, [-1, 7*7*64])\n",
    "    hypothesis = tf.matmul(L2, W3) + b\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    return hypothesis, cost\n",
    "\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis, cost = compute_cost(X, Y)\n",
    "    gradients  = tape.gradient(cost, [W1, W2, W3, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, W2, W3, b]))\n",
    "    return hypothesis, cost\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) / batch_size\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch, y_batch = batch\n",
    "        hypothesis, cost_val = train_step(x_batch, y_batch)\n",
    "        avg_cost += cost_val / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "hypothesis, cost_val = compute_cost(x_test, y_test,)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost 0.258709311\n",
      "Epoch: 0002 cost 0.074086808\n",
      "Epoch: 0003 cost 0.053217709\n",
      "Epoch: 0004 cost 0.044324409\n",
      "Epoch: 0005 cost 0.040001828\n",
      "Epoch: 0006 cost 0.034401406\n",
      "Epoch: 0007 cost 0.032023497\n",
      "Epoch: 0008 cost 0.027105700\n",
      "Epoch: 0009 cost 0.026817592\n",
      "Epoch: 0010 cost 0.026598470\n",
      "Epoch: 0011 cost 0.021547828\n",
      "Epoch: 0012 cost 0.020646309\n",
      "Epoch: 0013 cost 0.020462535\n",
      "Epoch: 0014 cost 0.019768706\n",
      "Epoch: 0015 cost 0.018117338\n",
      "Accuracy:  tf.Tensor(0.9932, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "x_train = tf.reshape(x_train, [-1, 28, 28, 1])\n",
    "x_test = tf.reshape(x_test, [-1, 28, 28, 1])\n",
    "y_train = tf.one_hot(y_train, depth=nb_classes)\n",
    "y_test = tf.one_hot(y_test, depth=nb_classes)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "dataset = train_data.batch(batch_size)\n",
    "\n",
    "# for features, label in dataset.take(1):\n",
    "    # print(features, label)\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "W1 = tf.Variable(initializer([3, 3, 1, 32]))\n",
    "\n",
    "W2 = tf.Variable(initializer([3, 3, 32, 64]))\n",
    "\n",
    "W3 = tf.Variable(initializer([3, 3, 64, 128]))\n",
    "\n",
    "W4 = tf.Variable(initializer([128*4*4, 625]))\n",
    "b4 = tf.Variable(tf.random.normal(([625])))\n",
    "\n",
    "W5 = tf.Variable(initializer([625, nb_classes]))\n",
    "b5 = tf.Variable(tf.random.normal([nb_classes]))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def compute_cost(X, Y, dropout_rate):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    \n",
    "    L1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    L1 = tf.nn.relu(L1)\n",
    "    L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    L1 = tf.nn.dropout(L1, rate = dropout_rate)\n",
    "    \n",
    "    L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    L2 = tf.nn.relu(L2)\n",
    "    L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    L2 = tf.nn.dropout(L2, rate = dropout_rate)\n",
    "    \n",
    "    L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding=\"SAME\")\n",
    "    L3 = tf.nn.relu(L3)\n",
    "    L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding=\"SAME\")\n",
    "    L3 = tf.nn.dropout(L3, rate = dropout_rate)\n",
    "    L3 = tf.reshape(L3, [-1, 128 * 4 * 4])\n",
    "    \n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    L4 - tf.nn.dropout(L4, rate = dropout_rate)\n",
    "    \n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    return hypothesis, cost\n",
    "\n",
    "def train_step(X, Y, dropout_rate):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis, cost = compute_cost(X, Y, dropout_rate)\n",
    "    gradients  = tape.gradient(cost, [W1, W2, W3, W4, W5, b4, b5])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, W2, W3, W4, W5, b4, b5]))\n",
    "    return hypothesis, cost\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) / batch_size\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch, y_batch = batch\n",
    "        hypothesis, cost_val = train_step(x_batch, y_batch, 0.3)\n",
    "        avg_cost += cost_val / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "hypothesis, cost_val = compute_cost(x_test, y_test, 0)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
