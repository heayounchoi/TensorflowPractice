{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enhancing performance with additional layers, initialization, and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost 1.375367522\n",
      "Epoch: 0002 cost 0.517982066\n",
      "Epoch: 0003 cost 0.431009620\n",
      "Epoch: 0004 cost 0.388565034\n",
      "Epoch: 0005 cost 0.362870783\n",
      "Epoch: 0006 cost 0.345713407\n",
      "Epoch: 0007 cost 0.333525181\n",
      "Epoch: 0008 cost 0.324450165\n",
      "Epoch: 0009 cost 0.317453653\n",
      "Epoch: 0010 cost 0.311925709\n",
      "Epoch: 0011 cost 0.307465553\n",
      "Epoch: 0012 cost 0.303797930\n",
      "Epoch: 0013 cost 0.300731212\n",
      "Epoch: 0014 cost 0.298131734\n",
      "Epoch: 0015 cost 0.295902938\n",
      "Accuracy:  tf.Tensor(0.9029, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "x_train = tf.reshape(x_train, [len(x_train), -1])\n",
    "x_test = tf.reshape(x_test, [len(x_test), -1])\n",
    "y_train = tf.one_hot(y_train, depth=nb_classes)\n",
    "y_test = tf.one_hot(y_test, depth=nb_classes)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "dataset = train_data.batch(batch_size)\n",
    "\n",
    "# for features, label in dataset.take(1):\n",
    "    # print(features, label)\n",
    "    \n",
    "W = tf.Variable(tf.random.normal([784, nb_classes]), name=\"weight\")\n",
    "b = tf.Variable(tf.random.normal([nb_classes]), name=\"bias\")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "def compute_cost(X, Y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    hypothesis = tf.matmul(X, W) + b\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    return hypothesis, cost\n",
    "\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis, cost = compute_cost(X, Y)\n",
    "    gradients  = tape.gradient(cost, [W, b])\n",
    "    optimizer.apply_gradients(zip(gradients, [W, b]))\n",
    "    return hypothesis, cost\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) / batch_size\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch, y_batch = batch\n",
    "        hypothesis, cost_val = train_step(x_batch, y_batch)\n",
    "        avg_cost += cost_val / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "hypothesis, cost_val = compute_cost(x_test, y_test)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost 167.822158813\n",
      "Epoch: 0002 cost 43.389518738\n",
      "Epoch: 0003 cost 27.534620285\n",
      "Epoch: 0004 cost 19.089965820\n",
      "Epoch: 0005 cost 13.877528191\n",
      "Epoch: 0006 cost 10.189608574\n",
      "Epoch: 0007 cost 7.579679966\n",
      "Epoch: 0008 cost 5.740392208\n",
      "Epoch: 0009 cost 4.331822395\n",
      "Epoch: 0010 cost 3.203842163\n",
      "Epoch: 0011 cost 2.430720329\n",
      "Epoch: 0012 cost 1.932021976\n",
      "Epoch: 0013 cost 1.568692446\n",
      "Epoch: 0014 cost 1.298140645\n",
      "Epoch: 0015 cost 1.088929176\n",
      "Accuracy:  tf.Tensor(0.947, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "x_train = tf.reshape(x_train, [len(x_train), -1])\n",
    "x_test = tf.reshape(x_test, [len(x_test), -1])\n",
    "y_train = tf.one_hot(y_train, depth=nb_classes)\n",
    "y_test = tf.one_hot(y_test, depth=nb_classes)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "dataset = train_data.batch(batch_size)\n",
    "\n",
    "# for features, label in dataset.take(1):\n",
    "    # print(features, label)\n",
    "    \n",
    "W1 = tf.Variable(tf.random.normal([784, 256]))\n",
    "b1 = tf.Variable(tf.random.normal([256]))\n",
    "\n",
    "W2 = tf.Variable(tf.random.normal([256, 256]))\n",
    "b2 = tf.Variable(tf.random.normal([256]))\n",
    "\n",
    "W3 = tf.Variable(tf.random.normal([256, nb_classes]))\n",
    "b3 = tf.Variable(tf.random.normal([nb_classes]))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def compute_cost(X, Y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    hypothesis = tf.matmul(L2, W3) + b3\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    return hypothesis, cost\n",
    "\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis, cost = compute_cost(X, Y)\n",
    "    gradients  = tape.gradient(cost, [W1, b2, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b2, W2, b2, W3, b3]))\n",
    "    return hypothesis, cost\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) / batch_size\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch, y_batch = batch\n",
    "        hypothesis, cost_val = train_step(x_batch, y_batch)\n",
    "        avg_cost += cost_val / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "hypothesis, cost_val = compute_cost(x_test, y_test)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost 0.306146532\n",
      "Epoch: 0002 cost 0.119249329\n",
      "Epoch: 0003 cost 0.077364348\n",
      "Epoch: 0004 cost 0.052916147\n",
      "Epoch: 0005 cost 0.037650805\n",
      "Epoch: 0006 cost 0.027615691\n",
      "Epoch: 0007 cost 0.024703706\n",
      "Epoch: 0008 cost 0.023192912\n",
      "Epoch: 0009 cost 0.016631611\n",
      "Epoch: 0010 cost 0.012814472\n",
      "Epoch: 0011 cost 0.014867225\n",
      "Epoch: 0012 cost 0.010770042\n",
      "Epoch: 0013 cost 0.010031405\n",
      "Epoch: 0014 cost 0.011199803\n",
      "Epoch: 0015 cost 0.008327252\n",
      "Accuracy:  tf.Tensor(0.9786, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "x_train = tf.reshape(x_train, [len(x_train), -1])\n",
    "x_test = tf.reshape(x_test, [len(x_test), -1])\n",
    "y_train = tf.one_hot(y_train, depth=nb_classes)\n",
    "y_test = tf.one_hot(y_test, depth=nb_classes)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "dataset = train_data.batch(batch_size)\n",
    "\n",
    "# for features, label in dataset.take(1):\n",
    "    # print(features, label)\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "W1 = tf.Variable(initializer([784, 256]))\n",
    "b1 = tf.Variable(tf.random.normal([256]))\n",
    "\n",
    "W2 = tf.Variable(initializer([256, 256]))\n",
    "b2 = tf.Variable(tf.random.normal([256]))\n",
    "\n",
    "W3 = tf.Variable(initializer([256, nb_classes]))\n",
    "b3 = tf.Variable(tf.random.normal([nb_classes]))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def compute_cost(X, Y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    hypothesis = tf.matmul(L2, W3) + b3\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    return hypothesis, cost\n",
    "\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis, cost = compute_cost(X, Y)\n",
    "    gradients  = tape.gradient(cost, [W1, b2, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b2, W2, b2, W3, b3]))\n",
    "    return hypothesis, cost\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) / batch_size\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch, y_batch = batch\n",
    "        hypothesis, cost_val = train_step(x_batch, y_batch)\n",
    "        avg_cost += cost_val / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "hypothesis, cost_val = compute_cost(x_test, y_test)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heayounchoi/anaconda3/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost 0.288661450\n",
      "Epoch: 0002 cost 0.111195326\n",
      "Epoch: 0003 cost 0.071078636\n",
      "Epoch: 0004 cost 0.055253364\n",
      "Epoch: 0005 cost 0.043906983\n",
      "Epoch: 0006 cost 0.036158655\n",
      "Epoch: 0007 cost 0.028299389\n",
      "Epoch: 0008 cost 0.027906612\n",
      "Epoch: 0009 cost 0.024974992\n",
      "Epoch: 0010 cost 0.021122567\n",
      "Epoch: 0011 cost 0.021207608\n",
      "Epoch: 0012 cost 0.018220656\n",
      "Epoch: 0013 cost 0.017939715\n",
      "Epoch: 0014 cost 0.015061238\n",
      "Epoch: 0015 cost 0.015217330\n",
      "Accuracy:  tf.Tensor(0.979, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "x_train = tf.reshape(x_train, [len(x_train), -1])\n",
    "x_test = tf.reshape(x_test, [len(x_test), -1])\n",
    "y_train = tf.one_hot(y_train, depth=nb_classes)\n",
    "y_test = tf.one_hot(y_test, depth=nb_classes)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "dataset = train_data.batch(batch_size)\n",
    "\n",
    "# for features, label in dataset.take(1):\n",
    "    # print(features, label)\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "W1 = tf.Variable(initializer([784, 512]))\n",
    "b1 = tf.Variable(tf.random.normal([512]))\n",
    "\n",
    "W2 = tf.Variable(initializer([512, 512]))\n",
    "b2 = tf.Variable(tf.random.normal([512]))\n",
    "\n",
    "W3 = tf.Variable(initializer([512, 512]))\n",
    "b3 = tf.Variable(tf.random.normal([512]))\n",
    "\n",
    "W4 = tf.Variable(initializer([512, 512]))\n",
    "b4 = tf.Variable(tf.random.normal([512]))\n",
    "\n",
    "W5 = tf.Variable(initializer([512, nb_classes]))\n",
    "b5 = tf.Variable(tf.random.normal([nb_classes]))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def compute_cost(X, Y):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
    "    L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
    "    hypothesis = tf.matmul(L4, W5) + b5\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    return hypothesis, cost\n",
    "\n",
    "def train_step(X, Y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis, cost = compute_cost(X, Y)\n",
    "    gradients  = tape.gradient(cost, [W1, b2, W2, b2, W3, b3, W4, b4, W5, b5])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b2, W2, b2, W3, b3, W4, b4, W5, b5]))\n",
    "    return hypothesis, cost\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) / batch_size\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch, y_batch = batch\n",
    "        hypothesis, cost_val = train_step(x_batch, y_batch)\n",
    "        avg_cost += cost_val / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "hypothesis, cost_val = compute_cost(x_test, y_test)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/heayounchoi/anaconda3/lib/python3.11/site-packages/keras/src/initializers/initializers.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initializer instance more than once.\n",
      "  warnings.warn(\n",
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 cost 0.425826013\n",
      "Epoch: 0002 cost 0.186285987\n",
      "Epoch: 0003 cost 0.134083822\n",
      "Epoch: 0004 cost 0.113129012\n",
      "Epoch: 0005 cost 0.094623961\n",
      "Epoch: 0006 cost 0.082894973\n",
      "Epoch: 0007 cost 0.074179210\n",
      "Epoch: 0008 cost 0.067577019\n",
      "Epoch: 0009 cost 0.061436187\n",
      "Epoch: 0010 cost 0.055878833\n",
      "Epoch: 0011 cost 0.051597685\n",
      "Epoch: 0012 cost 0.049780294\n",
      "Epoch: 0013 cost 0.043976586\n",
      "Epoch: 0014 cost 0.042386804\n",
      "Epoch: 0015 cost 0.042665269\n",
      "Accuracy:  tf.Tensor(0.9813, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "nb_classes = 10\n",
    "x_train = tf.reshape(x_train, [len(x_train), -1])\n",
    "x_test = tf.reshape(x_test, [len(x_test), -1])\n",
    "y_train = tf.one_hot(y_train, depth=nb_classes)\n",
    "y_test = tf.one_hot(y_test, depth=nb_classes)\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "training_epochs = 15\n",
    "batch_size = 100\n",
    "\n",
    "dataset = train_data.batch(batch_size)\n",
    "\n",
    "# for features, label in dataset.take(1):\n",
    "    # print(features, label)\n",
    "\n",
    "initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "W1 = tf.Variable(initializer([784, 256]))\n",
    "b1 = tf.Variable(tf.random.normal([256]))\n",
    "\n",
    "W2 = tf.Variable(initializer([256, 256]))\n",
    "b2 = tf.Variable(tf.random.normal([256]))\n",
    "\n",
    "W3 = tf.Variable(initializer([256, nb_classes]))\n",
    "b3 = tf.Variable(tf.random.normal([nb_classes]))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "def compute_cost(X, Y, keep_prob):\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
    "    L1 = tf.nn.dropout(L1, rate=keep_prob)\n",
    "    L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
    "    L2 = tf.nn.dropout(L2, rate=keep_prob)\n",
    "    hypothesis = tf.matmul(L2, W3) + b3\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=hypothesis, labels=Y))\n",
    "    return hypothesis, cost\n",
    "\n",
    "def train_step(X, Y, keep_prob):\n",
    "    with tf.GradientTape() as tape:\n",
    "        hypothesis, cost = compute_cost(X, Y, keep_prob)\n",
    "    gradients  = tape.gradient(cost, [W1, b2, W2, b2, W3, b3])\n",
    "    optimizer.apply_gradients(zip(gradients, [W1, b2, W2, b2, W3, b3]))\n",
    "    return hypothesis, cost\n",
    "\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0\n",
    "    total_batch = len(train_data) / batch_size\n",
    "\n",
    "    for batch in dataset:\n",
    "        x_batch, y_batch = batch\n",
    "        hypothesis, cost_val = train_step(x_batch, y_batch, keep_prob=0.3)\n",
    "        avg_cost += cost_val / total_batch\n",
    "\n",
    "    print(\"Epoch:\", \"%04d\" % (epoch + 1), \"cost\", \"{:.9f}\".format(avg_cost))\n",
    "\n",
    "hypothesis, cost_val = compute_cost(x_test, y_test, 0)\n",
    "is_correct = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(y_test, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
